model: null
dataset: null
adapter: adapter.npz
prompt: null
resume: false
seed: 0

quantize: null

mode: lora
fuse: true
lora:
  layers: 16
  rank: 8

optimizer:
  _target_: mlx.optimizers.Adam
  learning_rate: 0.00001

train:
  batch_size: 2
  itrs: 10
  save_every: 5
  steps_per_eval: 10
  steps_per_report: 2

validation:
  batches: 32

test:
  batches: 512
  batch_size: 2
  sample:
    temperature: 1.0
    max_tokens: 2048

defaults:
  - hydra.yaml
